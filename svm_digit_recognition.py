"""
ğŸ”¢ MÃ´ hÃ¬nh SVM Nháº­n dáº¡ng Chá»¯ sá»‘ Viáº¿t tay (MNIST)

Script nÃ y triá»ƒn khai Ä‘áº§y Ä‘á»§ quy trÃ¬nh xÃ¢y dá»±ng mÃ´ hÃ¬nh SVM Ä‘á»ƒ nháº­n dáº¡ng 
chá»¯ sá»‘ viáº¿t tay sá»­ dá»¥ng bá»™ dá»¯ liá»‡u MNIST.

CÃ¡c bÆ°á»›c chÃ­nh:
1. Chuáº©n bá»‹ dá»¯ liá»‡u vÃ  tiá»n xá»­ lÃ½
2. Huáº¥n luyá»‡n mÃ´ hÃ¬nh SVM
3. ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh
4. Tá»‘i Æ°u hÃ³a mÃ´ hÃ¬nh (GridSearch, PCA)
5. Xuáº¥t Ä‘áº§u ra cho há»‡ ensemble

Sá»­ dá»¥ng:
    python svm_digit_recognition.py

TÃ¡c giáº£: AI Assistant
NgÃ y táº¡o: 2024
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from time import time
import warnings
import argparse
import os

warnings.filterwarnings('ignore')

# Sklearn imports
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedShuffleSplit
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.svm import SVC
from sklearn.metrics import (
    classification_report,
    confusion_matrix,
    accuracy_score,
)
import joblib

# =============================================================================
# Cáº¤U HÃŒNH
# =============================================================================

class Config:
    """Cáº¥u hÃ¬nh cho mÃ´ hÃ¬nh."""
    
    # Dá»¯ liá»‡u
    TEST_SIZE = 10000
    RANDOM_STATE = 42
    USE_SUBSET = False  # Sá»­ dá»¥ng toÃ n bá»™ dá»¯ liá»‡u train Ä‘á»ƒ Ä‘áº¡t accuracy cao nháº¥t
    SUBSET_SIZE = 60000  # Sá»‘ máº«u train (60000 = full MNIST train)
    
    # SVM - Tham sá»‘ tá»‘i Æ°u cho MNIST
    DEFAULT_KERNEL = 'rbf'
    DEFAULT_C = 10.0  # Tá»‘i Æ°u cho MNIST (thay vÃ¬ 1.0)
    DEFAULT_GAMMA = 0.01  # Tá»‘i Æ°u cho MNIST (thay vÃ¬ 'scale')
    
    # GridSearch
    GRID_SEARCH_SAMPLES = 5000
    GRID_CV = 3
    
    # PCA
    USE_PCA = False
    PCA_COMPONENTS = 100
    
    # Output
    OUTPUT_DIR = 'outputs'
    MODEL_FILENAME = 'svm_digit_classifier.joblib'
    PREDICTIONS_FILENAME = 'svm_predictions_for_ensemble.csv'


# =============================================================================
# HÃ€M TIá»†N ÃCH
# =============================================================================

def check_gpu():
    """Kiá»ƒm tra GPU vÃ  cuML."""
    try:
        import subprocess
        result = subprocess.run(['nvidia-smi'], capture_output=True, text=True)
        gpu_available = True
    except:
        gpu_available = False
    
    try:
        from cuml.svm import SVC as cuSVC
        cuml_available = True
    except ImportError:
        cuml_available = False
    
    return gpu_available, cuml_available


def load_mnist():
    """Táº£i vÃ  tiá»n xá»­ lÃ½ dá»¯ liá»‡u MNIST."""
    print("ğŸ“¥ Äang táº£i dá»¯ liá»‡u MNIST...")
    start_time = time()
    
    X, y = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False, parser='auto')
    
    print(f"âœ… Táº£i xong trong {time() - start_time:.2f} giÃ¢y")
    print(f"\nğŸ“Š ThÃ´ng tin dá»¯ liá»‡u:")
    print(f"   - Shape cá»§a X: {X.shape}")
    print(f"   - Shape cá»§a y: {y.shape}")
    print(f"   - Sá»‘ lÆ°á»£ng lá»›p: {len(np.unique(y))}")
    
    # Chuyá»ƒn Ä‘á»•i
    y = y.astype(int)
    X = X.astype(np.float32) / 255.0
    
    print(f"   - Range sau chuáº©n hÃ³a: [{X.min():.2f}, {X.max():.2f}]")
    
    return X, y


def split_data(X, y, config):
    """Chia dá»¯ liá»‡u train/test."""
    X_train, X_test, y_train, y_test = train_test_split(
        X, y,
        test_size=config.TEST_SIZE,
        random_state=config.RANDOM_STATE,
        stratify=y
    )
    
    print(f"\nğŸ“Š Chia dá»¯ liá»‡u:")
    print(f"   - Train: {X_train.shape[0]} máº«u")
    print(f"   - Test: {X_test.shape[0]} máº«u")
    
    # Sá»­ dá»¥ng táº­p con náº¿u cáº§n
    if config.USE_SUBSET:
        print(f"\nâš¡ Sá»­ dá»¥ng táº­p con {config.SUBSET_SIZE} máº«u...")
        sss = StratifiedShuffleSplit(n_splits=1, train_size=config.SUBSET_SIZE, 
                                      random_state=config.RANDOM_STATE)
        for train_idx, _ in sss.split(X_train, y_train):
            X_train = X_train[train_idx]
            y_train = y_train[train_idx]
        print(f"   - Táº­p train subset: {X_train.shape[0]} máº«u")
    
    return X_train, X_test, y_train, y_test


def create_pipeline(kernel='rbf', C=10.0, gamma=0.01, use_pca=False, n_components=100):
    """
    Táº¡o pipeline cho SVM.
    
    LÆ¯U Ã QUAN TRá»ŒNG: KHÃ”NG dÃ¹ng StandardScaler trong pipeline.
    Dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a vá» [0, 1] báº±ng cÃ¡ch chia cho 255.
    Äiá»u nÃ y giÃºp Ä‘áº£m báº£o tÃ­nh nháº¥t quÃ¡n khi dá»± Ä‘oÃ¡n áº£nh má»›i.
    """
    steps = []
    
    # KHÃ”NG dÃ¹ng StandardScaler - sá»­ dá»¥ng chuáº©n hÃ³a 0-1 Ä‘Æ¡n giáº£n thay tháº¿
    # Äiá»u nÃ y trÃ¡nh váº¥n Ä‘á» khÃ´ng khá»›p khi dá»± Ä‘oÃ¡n áº£nh má»›i
    
    if use_pca:
        steps.append(('pca', PCA(n_components=n_components)))
    
    steps.append(('svc', SVC(
        kernel=kernel,
        C=C,
        gamma=gamma,
        probability=True,
        cache_size=2000,
        random_state=42
    )))
    
    # Náº¿u khÃ´ng dÃ¹ng PCA, tráº£ vá» SVC trá»±c tiáº¿p
    if len(steps) == 1:
        return steps[0][1]
    
    return Pipeline(steps)


def train_svm(X_train, y_train, kernel='rbf', C=10.0, gamma=0.01,
              use_pca=False, n_components=100):
    """Huáº¥n luyá»‡n mÃ´ hÃ¬nh SVM."""
    print(f"\nğŸ‹ï¸ Báº¯t Ä‘áº§u huáº¥n luyá»‡n SVM...")
    print(f"   - Kernel: {kernel}")
    print(f"   - C: {C}")
    print(f"   - Gamma: {gamma}")
    print(f"   - PCA: {use_pca} ({n_components} components)" if use_pca else f"   - PCA: {use_pca}")
    print(f"   - Sá»‘ máº«u train: {len(X_train)}")
    
    model = create_pipeline(kernel, C, gamma, use_pca, n_components)
    
    start_time = time()
    model.fit(X_train, y_train)
    train_time = time() - start_time
    
    print(f"\nâœ… Huáº¥n luyá»‡n hoÃ n táº¥t trong {train_time:.2f} giÃ¢y")
    
    # Hiá»ƒn thá»‹ sá»‘ support vectors náº¿u cÃ³
    if hasattr(model, 'n_support_'):
        print(f"   - Sá»‘ support vectors: {sum(model.n_support_)}")
    elif hasattr(model, 'named_steps') and hasattr(model.named_steps['svc'], 'n_support_'):
        print(f"   - Sá»‘ support vectors: {sum(model.named_steps['svc'].n_support_)}")
    
    return model, train_time


def evaluate_model(model, X_test, y_test, model_name="Model"):
    """ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh."""
    print(f"\n{'='*60}")
    print(f"ğŸ“Š ÄÃ¡nh giÃ¡: {model_name}")
    print(f"{'='*60}")
    
    # Dá»± Ä‘oÃ¡n
    start_time = time()
    y_pred = model.predict(X_test)
    predict_time = time() - start_time
    
    # TÃ­nh accuracy
    accuracy = accuracy_score(y_test, y_pred)
    
    print(f"\nğŸ¯ Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
    print(f"â±ï¸ Thá»i gian dá»± Ä‘oÃ¡n: {predict_time:.4f} giÃ¢y")
    
    # Classification report
    print(f"\nğŸ“‹ Classification Report:")
    print(classification_report(y_test, y_pred, digits=4))
    
    # Confusion matrix
    cm = confusion_matrix(y_test, y_pred)
    
    return {
        'accuracy': accuracy,
        'predict_time': predict_time,
        'y_pred': y_pred,
        'confusion_matrix': cm
    }


def run_grid_search(X_train, y_train, config):
    """Thá»±c hiá»‡n Grid Search Ä‘á»ƒ tÃ¬m tham sá»‘ tá»‘t nháº¥t."""
    print("\nğŸ” Báº¯t Ä‘áº§u Grid Search...")
    print("âš ï¸ QuÃ¡ trÃ¬nh nÃ y cÃ³ thá»ƒ máº¥t vÃ i phÃºt...\n")
    
    # Sá»­ dá»¥ng SVC trá»±c tiáº¿p thay vÃ¬ pipeline (khÃ´ng dÃ¹ng StandardScaler)
    svc = SVC(probability=True, cache_size=2000, random_state=42)
    
    # LÆ°á»›i tham sá»‘ tá»‘i Æ°u cho MNIST
    param_grid = {
        'C': [1, 5, 10],
        'gamma': [0.01, 0.02, 0.05],
        'kernel': ['rbf']
    }
    
    # Sá»­ dá»¥ng táº­p con cho GridSearch
    n_samples = min(config.GRID_SEARCH_SAMPLES, len(X_train))
    X_grid = X_train[:n_samples]
    y_grid = y_train[:n_samples]
    
    print(f"ğŸ“Š Sá»­ dá»¥ng {n_samples} máº«u cho GridSearch")
    
    grid_search = GridSearchCV(
        svc,
        param_grid,
        cv=config.GRID_CV,
        n_jobs=-1,
        verbose=2,
        scoring='accuracy',
        return_train_score=True
    )
    
    start_time = time()
    grid_search.fit(X_grid, y_grid)
    grid_time = time() - start_time
    
    print(f"\nâœ… GridSearch hoÃ n táº¥t trong {grid_time:.2f} giÃ¢y")
    print(f"\nğŸ“Š Káº¿t quáº£ GridSearch:")
    print(f"   - Best Score (CV): {grid_search.best_score_:.4f}")
    print(f"   - Best Parameters: {grid_search.best_params_}")
    
    # Chuyá»ƒn Ä‘á»•i key Ä‘á»ƒ tÆ°Æ¡ng thÃ­ch vá»›i code cÅ©
    best_params = {
        'svc__kernel': grid_search.best_params_['kernel'],
        'svc__C': grid_search.best_params_['C'],
        'svc__gamma': grid_search.best_params_['gamma']
    }
    
    return best_params


def plot_confusion_matrix(cm, output_path):
    """Váº½ vÃ  lÆ°u ma tráº­n nháº§m láº«n."""
    plt.figure(figsize=(12, 10))
    
    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    
    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',
                xticklabels=range(10), yticklabels=range(10),
                cbar_kws={'label': 'Tá»· lá»‡'})
    
    plt.title('Ma tráº­n Nháº§m láº«n (Normalized) - MÃ´ hÃ¬nh SVM', fontsize=14)
    plt.xlabel('Dá»± Ä‘oÃ¡n', fontsize=12)
    plt.ylabel('Thá»±c táº¿', fontsize=12)
    plt.tight_layout()
    plt.savefig(output_path, dpi=150, bbox_inches='tight')
    plt.close()
    print(f"âœ… ÄÃ£ lÆ°u: {output_path}")


def save_outputs(model, X_test, y_test, results, config):
    """LÆ°u mÃ´ hÃ¬nh vÃ  Ä‘áº§u ra."""
    os.makedirs(config.OUTPUT_DIR, exist_ok=True)
    
    print("\nğŸ’¾ LÆ°u Ä‘áº§u ra...")
    
    # Láº¥y xÃ¡c suáº¥t vÃ  dá»± Ä‘oÃ¡n
    proba = model.predict_proba(X_test)
    pred = results['y_pred']
    
    # LÆ°u mÃ´ hÃ¬nh
    model_path = os.path.join(config.OUTPUT_DIR, config.MODEL_FILENAME)
    joblib.dump(model, model_path)
    print(f"âœ… ÄÃ£ lÆ°u mÃ´ hÃ¬nh: {model_path}")
    
    # LÆ°u predictions cho ensemble
    ensemble_output = pd.DataFrame(proba, columns=[f'prob_digit_{i}' for i in range(10)])
    ensemble_output['predicted_label'] = pred
    ensemble_output['true_label'] = y_test
    
    csv_path = os.path.join(config.OUTPUT_DIR, config.PREDICTIONS_FILENAME)
    ensemble_output.to_csv(csv_path, index=False)
    print(f"âœ… ÄÃ£ lÆ°u: {csv_path}")
    
    # LÆ°u numpy arrays
    np.save(os.path.join(config.OUTPUT_DIR, 'svm_probabilities.npy'), proba)
    np.save(os.path.join(config.OUTPUT_DIR, 'svm_predictions.npy'), pred)
    print(f"âœ… ÄÃ£ lÆ°u numpy arrays")
    
    # LÆ°u confusion matrix
    cm_path = os.path.join(config.OUTPUT_DIR, 'confusion_matrix.png')
    plot_confusion_matrix(results['confusion_matrix'], cm_path)
    
    return proba, pred


def predict_digit(model, image):
    """
    Dá»± Ä‘oÃ¡n chá»¯ sá»‘ tá»« áº£nh.
    
    Parameters:
    -----------
    model : sklearn Pipeline
        MÃ´ hÃ¬nh SVM Ä‘Ã£ huáº¥n luyá»‡n
    image : array-like
        áº¢nh Ä‘áº§u vÃ o (28x28 hoáº·c 784,)
        
    Returns:
    --------
    dict : Káº¿t quáº£ dá»± Ä‘oÃ¡n
    """
    # Flatten náº¿u cáº§n
    if image.ndim == 2:
        image = image.reshape(1, -1)
    elif image.ndim == 1:
        image = image.reshape(1, -1)
    
    # Chuáº©n hÃ³a náº¿u cáº§n
    if image.max() > 1:
        image = image.astype(np.float32) / 255.0
    
    # Dá»± Ä‘oÃ¡n
    pred = model.predict(image)[0]
    proba = model.predict_proba(image)[0]
    
    return {
        'prediction': pred,
        'confidence': proba[pred],
        'probabilities': proba
    }


# =============================================================================
# MAIN
# =============================================================================

def main():
    """HÃ m chÃ­nh."""
    parser = argparse.ArgumentParser(description='SVM Digit Recognition')
    parser.add_argument('--subset-size', type=int, default=10000,
                        help='Sá»‘ máº«u train Ä‘á»ƒ sá»­ dá»¥ng (default: 10000)')
    parser.add_argument('--use-full-data', action='store_true',
                        help='Sá»­ dá»¥ng toÃ n bá»™ dá»¯ liá»‡u train')
    parser.add_argument('--skip-grid-search', action='store_true',
                        help='Bá» qua Grid Search')
    parser.add_argument('--kernel', type=str, default='rbf',
                        choices=['rbf', 'linear', 'poly', 'sigmoid'],
                        help='Loáº¡i kernel SVM (default: rbf)')
    parser.add_argument('--C', type=float, default=1.0,
                        help='Há»‡ sá»‘ regularization C (default: 1.0)')
    parser.add_argument('--use-pca', action='store_true',
                        help='Sá»­ dá»¥ng PCA giáº£m chiá»u')
    parser.add_argument('--pca-components', type=int, default=100,
                        help='Sá»‘ thÃ nh pháº§n PCA (default: 100)')
    
    args = parser.parse_args()
    
    # Cáº¥u hÃ¬nh
    config = Config()
    config.SUBSET_SIZE = args.subset_size
    config.USE_SUBSET = not args.use_full_data
    config.USE_PCA = args.use_pca
    config.PCA_COMPONENTS = args.pca_components
    
    print("="*60)
    print("ğŸ”¢ MÃ” HÃŒNH SVM NHáº¬N Dáº NG CHá»® Sá» VIáº¾T TAY")
    print("="*60)
    
    # Kiá»ƒm tra GPU
    gpu_available, cuml_available = check_gpu()
    print(f"\nğŸ“Š Cáº¥u hÃ¬nh:")
    print(f"   - GPU Available: {gpu_available}")
    print(f"   - cuML Available: {cuml_available}")
    
    # 1. Táº£i dá»¯ liá»‡u
    print("\n" + "="*60)
    print("ğŸ“¦ BÆ¯á»šC 1: Chuáº©n bá»‹ dá»¯ liá»‡u")
    print("="*60)
    
    X, y = load_mnist()
    X_train, X_test, y_train, y_test = split_data(X, y, config)
    
    # 2. Grid Search (tÃ¹y chá»n)
    if not args.skip_grid_search:
        print("\n" + "="*60)
        print("âš™ï¸ BÆ¯á»šC 2: Tá»‘i Æ°u hÃ³a siÃªu tham sá»‘")
        print("="*60)
        
        best_params = run_grid_search(X_train, y_train, config)
        kernel = best_params['svc__kernel']
        C = best_params['svc__C']
        gamma = best_params.get('svc__gamma', 'scale')
    else:
        kernel = args.kernel
        C = args.C
        gamma = config.DEFAULT_GAMMA
    
    # 3. Huáº¥n luyá»‡n mÃ´ hÃ¬nh cuá»‘i cÃ¹ng
    print("\n" + "="*60)
    print("ğŸ‹ï¸ BÆ¯á»šC 3: Huáº¥n luyá»‡n mÃ´ hÃ¬nh cuá»‘i cÃ¹ng")
    print("="*60)
    
    final_model, train_time = train_svm(
        X_train, y_train,
        kernel=kernel,
        C=C,
        gamma=gamma,
        use_pca=config.USE_PCA,
        n_components=config.PCA_COMPONENTS
    )
    
    # 4. ÄÃ¡nh giÃ¡
    print("\n" + "="*60)
    print("ğŸ“Š BÆ¯á»šC 4: ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh")
    print("="*60)
    
    results = evaluate_model(final_model, X_test, y_test, "MÃ´ hÃ¬nh Cuá»‘i cÃ¹ng")
    
    # 5. LÆ°u Ä‘áº§u ra
    print("\n" + "="*60)
    print("ğŸ’¾ BÆ¯á»šC 5: LÆ°u Ä‘áº§u ra")
    print("="*60)
    
    save_outputs(final_model, X_test, y_test, results, config)
    
    # Tá»•ng káº¿t
    print("\n" + "="*60)
    print("ğŸ“Š Tá»”NG Káº¾T")
    print("="*60)
    print(f"\nğŸ¯ Accuracy: {results['accuracy']:.4f} ({results['accuracy']*100:.2f}%)")
    print(f"â±ï¸ Thá»i gian huáº¥n luyá»‡n: {train_time:.2f} giÃ¢y")
    print(f"\nğŸ“ CÃ¡c file Ä‘Ã£ lÆ°u trong thÆ° má»¥c '{config.OUTPUT_DIR}/':")
    print(f"   - {config.MODEL_FILENAME}")
    print(f"   - {config.PREDICTIONS_FILENAME}")
    print(f"   - svm_probabilities.npy")
    print(f"   - svm_predictions.npy")
    print(f"   - confusion_matrix.png")
    print("\nâœ… HoÃ n táº¥t!")
    
    return final_model, results


if __name__ == "__main__":
    model, results = main()
